\documentclass{article}

\usepackage{algorithm2e}
\usepackage{algpseudocode}
%\usepackage{algorithmicx}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{cite}
\usepackage{enumerate}
%\usepackage{enumitem}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{mathpartir}
\usepackage{stmaryrd}
\usepackage{subcaption}
\usepackage[usenames,dvipsnames]{xcolor}
\usepackage{xstring}
\usepackage{tikz}
\usetikzlibrary{matrix}

\newcommand\eg{\textit{e.g.}}
\newcommand\ie{\textit{i.e.}}

\newcommand{\R}{\mathbb{R}}
\newcommand\mC{\mathcal{C}}
\newcommand\mD{\mathcal{D}}
\newcommand\mE{\mathcal{E}}
\newcommand\mI{\mathcal{I}}
\newcommand\mN{\mathcal{N}}
\newcommand\mM{\mathcal{M}}
\newcommand\mT{\mathcal{T}}

\newcommand{\eqdef}[0]{\triangleq}

\newcommand\bnfsep{\,\, | \,\,}

\newcommand\evar[1]{?#1}
\newcommand\typ{\tau}
\newcommand\ctx{\Gamma}
\newcommand\glob{\Upsilon}
\newcommand\const{\mC}
\newcommand\conid{\mT}
\newcommand\ind{\Upsilon}

\newcommand\denote[1]{\llbracket#1\rrbracket}
\newcommand\env{\rho}
\newcommand\embed[1]{\mE\denote{#1}}
\newcommand\embedenv[1]{\mE\denote{#1}\env}
\newcommand\combine{\oplus}
\newcommand\modelenv[1]{\mM\denote{#1}\env}

\begin{document}


\section{Introduction}

We are interested in constructing a \emph{position evaluator} for Coq
proofs. More concretely, we want to learn a function
\[
V: \vec{TS} \rightarrow [0, \infty) \,,
  \]
  where $\vec{TS}$ is a sequence of \emph{tactic state} and the
  codomain $[0, \infty)$ is a scalar metric that evaluates the provability
    of the current tactic state. A lower number means that we believe
    the state is provable and a number diverging towards $\infty$
    means that tactic static is not provable.

We believe that such a function will be difficult to
learn. Nevertheless, we believe the diffuclty of the problem to be in
a ``sweet-spot''.
\begin{enumerate}
\item Position evaluation is more difficult to solve than premise
  selection. In premise selection, the idea is to select from a finite
  set of lemmas those that will be useful in proving the current
  conjecture. A position evaluator should assign a high number to
  tactic states which do not contain the requisite lemmas required to
  prove the current goal.
\item Position evaluation is easier to solve than tactic
  prediction. In tactic prediction, the idea is to select a tactic to
  apply to the current tactic state that will advance us towards
  proving the current conjecture. For simple inferences, very fast
  decision procedures already exist, undermining the usefulness of
  tactic prediction in simple cases. For complex inferences, such as
  synthesizing existentital terms, the problem is already out of reach
  for most automation techniques.
\end{enumerate}
We also believe that such a function will be an important building
block for advancing the level of automation in interactive theorem
provers (ITPs), and thus, worthwhile to attempt solving.
\begin{enumerate}
\item Position evaluation, if trained well, can be deployed inside of
  interative proof assistants to aid the human user in determining if
  the current proof state looks promising (\ie, a low number), or if
  key invariants and/or definitions are missing (\ie, diverging towards
  $\infty$).
\item A position evaluation artifact can be combined with (fast)
  enumerative search to prune the search space. For instance, one
  could imagine applying the position evaluator to branches deep
  within the search tree and display to the human user the top-k most
  promising branches of an enumerative search to aid the human in
  synthesizing an existential-term.
\item In a reinforcement learning setting, we can imagine the position
  evaluator to be the value function, which could use to construct a
  policy that suggests tactics to apply. (We could imagine learning
  the tactic directly, corresponding to learning the policy directly,
  but this produces an artifact that is not easily usable outside of
  the reinforcement learning framework.)
\end{enumerate}
In short, we believe that learning a position evaluator is a difficult
problem to solve that also produces a potentially useful artifact.


  
\section{Syntax}

\subsection{Terms}

We are working with Coq's implementation of the Calculus of Inductive
Constructions (CIC). (Note that terms are types.)
\begin{align*}
  M & ::= c \bnfsep \texttt{C} \, \texttt{I} \, i \bnfsep \texttt{I} \, i: \typ \\
  & \quad \bnfsep x \bnfsep \lambda x: \typ. M \bnfsep M_1 \, M_2 \\
  & \quad \dots \\
  \typ & ::= M
\end{align*}


\subsection{Tactic State}

A \emph{tactic state} is comprised of a local context $\ctx$, a global
context $\glob$, and a goal (\ie, an existential term $\evar{n}$ that
we are trying to instatiate). The global context is usually left
implicit.
\begin{align*}
  TS & ::= \ctx \vdash \evar{n}: typ 
\end{align*}
When we apply a tactic, we are asking Coq if it can instantiate an
inference rule of the form below. The theorem we are proving is
encoded in the type $\typ$.
\begin{mathpar}
  \inferrule*[right={tactic}]
      {\glob; \ctx \eqdef x_1: \typ_1, \dots, x_n: \typ_n}
      {\evar{n}: \typ}
\end{mathpar}


\subsection{Tactic Trees and Proofs}

Suppose we run a tactic $T$ on a tactic state $\ctx \vdash \evar{n}:
\typ$, generating subgoals $\evar{m_1}, \dots, \evar{m_q}$,
abbreviated:
\begin{align*}
  \ctx \vdash \evar{n}: \typ \rightarrow_T \evar{m_1}, \dots, \evar{m_q}
\end{align*}
This corresponds to a tree structure with parent node $\ctx \vdash
\evar{n}: \typ$ and children nodes $\ctx_1 \vdash \evar{m_1}: \typ_1,
\dots, \ctx_1 \vdash \evar{m_q}: \typ_q$, one for each subgoal. We refer to
this tree of tactic states as a \emph{tactic tree}.
\begin{align*}
  TR & ::= \texttt{T}(TS) \bnfsep \texttt{C}(TS; TS_1, \dots, TS_m)
\end{align*}
A terminal tactic state $\texttt{T}(TS)$ contains a terminal tactic
state $TS$ and has no children, while a non-terminal tactic state
$\texttt{C}(TS; TS_1, \dots, TS_m)$ has parent tactic state $TS$ and
children tactic states $TS_1, \dots, TS_m$.

 For example, if we apply the induction principle for natural numbers,
 we will create two new existential terms, one for the base case
 ($\evar{p}$) and one for the inductive case ($\evar{q}$).
\begin{mathpar}
  \inferrule*[right={induction}]
      {\glob; \ctx}
      {\evar{n}: \typ}
  \\
  \inferrule*[right={tactic}]
      {\glob; \ctx_p}
      {\evar{p}: \typ_p}
  \and
  \inferrule*[right={tactic}]
      {\glob; \ctx_q}
      {\evar{q}: \typ_q}
\end{mathpar}

A \emph{proof} in Coq corresponds to a tactic tree which has all its
existential terms instantiated to concrete terms. We will also refer
to such a tactic tree as a \emph{well-typed tactic tree}.


\section{Model}

As a reminder, we are interested in constructing a \emph{position
  evaluator} for Coq proofs, \ie, a function $V: \vec{TS} \rightarrow
[0, \infty)$ that is a proxy for the ``goodness'' of a tactic
  state. The first step is to represent tactic tree as a vector in $D$
  dimensional space. The second step is to build a model that uses the
  remaining distance to a terminal node in the tree as a proxy for the
  ``goodness'' of a tactic state. Throughout this section, we assume
  we are embedding in a vector space $\R^D$. The notation $v \sim
  \mN(0, \mI)$ means that we are sampling a random vector from a
  standard multivariate normal distribution (in $D$ dimensions).


\subsection{Embedding Terms}

The embeddings are parameterized by global embedding contexts for
constants ($\const$), constructors ($\conid$), and inductive types
($\ind$). (Also have sorts, evars, fixpoints, and co-fixpoints.) The
embedding is defined by induction on the typing derivation and uses
the combination operator $\combine: \R^D \rightarrow \R^D \rightarrow \R^D$.
\begin{align*}
  \embedenv{\ctx \vdash c: \typ} & \eqdef \const(c) \\
  \embedenv{\ctx \vdash \texttt{C} \, \texttt{I} \, i: \typ} & \eqdef \conid(\texttt{C}, i) \\
  \embedenv{\ctx \vdash \texttt{I} \, i: \typ} & \eqdef \ind(\texttt{I}, i) \\
  \embedenv{\ctx \vdash x: \typ} & \eqdef \env(x) \\
  \embedenv{\ctx \vdash \lambda x: \typ. M: \typ_1 \rightarrow \typ_2} & \eqdef \embedenv{\ctx, x: \typ_1 \vdash M: \typ_2}[x \mapsto v] \\
  & \quad \mbox{where $v \sim \mN(0, \mI)$} \\
  \embedenv{\ctx \vdash M_1 \, M_2: \typ} & \eqdef \embedenv{\ctx \vdash M_1: \typ_2 \rightarrow \typ} \combine \embedenv{\ctx \vdash M_2: \typ_2} \\
  \dots & \eqdef \dots
\end{align*}

NOTE(Dan): With an embedding defined by induction on the typing
derivation, we should be able to use different spaces to represent
different types. I would argue this is actually the \emph{natural way}
to view embedding from a linguistic perspective. For instance, the
embedding of a function type should be a \emph{matrix} to make an
analogy with function analysis. This introduces a lot more parameters
to learn, but that should be expected because functions have much more
degrees of freedom than base types such as natural numbers. To start,
we should use standard embeddings, but I suspect that much can be
gained by using different spaces to embed different types.


\subsection{Embedding Tactic States}

We embed the local context by induction on the structure of
contexts.
\begin{align*}
  \embedenv{\cdot} & \eqdef v_\emptyset \\
  & \quad\mbox{where $v_\emptyset$ is the embedding of the empty context ``token''} \\
  \embedenv{x: \typ_1, \ctx} & \eqdef v \rhd \embedenv{\ctx}[x \mapsto v] \\
  & \quad\mbox{where $v = \embedenv{\typ_1}$}
\end{align*}
The structure that we would like the embedding to capture is an
ordered sequence of types. (The ordering is important because we have
dependent types.) We use the combinator $\rhd: \R^D \rightarrow
\texttt{M} \, \R^D$ to denote composition by LSTM (for the appropriate
monad $\texttt{M}$). We should also investigate other architectures
which are suitable for capturing sequences, but we imagine that
vanilla LSTMs will be a good baseline for embedding proof contexts. We
also believe that an attention mechanism, \ie, determining which
hypotheses to focus on in the current tactic state, will also be
valuable.

We embed a tactic state by induction on the structure of tactic
states. Here, we simply embed the context with the context embedding
and embed the goal separately.
\begin{align*}
  \embedenv{\ctx \vdash \evar{n}: \typ} & \eqdef (\embedenv{\ctx}, \embedenv{\typ}[\embedenv{\ctx}])
\end{align*}


\subsection{Position evaluation}

Suppose we have a well-typed tactic tree $TR$. We give an example
construction of a position evaluation model by induction on the
structure of the tactic tree as below. We imagine there to interesting
architectural decisions to be made in this part.
\begin{align*}
  \embedenv{\texttt{T}(TS)} & \eqdef \embedenv{TS} \\
  \embedenv{\texttt{C}(TS; TS_1, \dots, TS_m), \ell} & \eqdef ((c, g) \rhd_2 \embedenv{TS_1}, \dots, (c, g) \rhd_2 \embedenv{TS_m}) \\
  & \mbox{where $(c, g) = \embedenv{TS}$}
\end{align*}
The operator $\rhd_2$ is defined as
\begin{align*}
  (c_1, g_1) \rhd_2 (c_2, g_2) & \eqdef d_2 \\
  & \quad \mbox{where $d_2 = A(c_1 \rhd c_2, g_1 \rhd g_2)$} \,,
\end{align*}
where $\mD: \R^D \times \R^D \rightarrow \R^D$ takes the hidden state for the context
LSTM and the goal LSTM and produces a vector. The idea behind $\mD$ is
that we want to ``diff'' the context and goal for successive tactic
states.  In some sense, the goal is a compressed version of the
context. (Or conversely, we should be able to decompress a goal into a
context.) The output of the diff is what we will use to train our
model. Note that we still run two LSTMs over the contexts and goals
separately. The idea is that we want our model to keep track of the
sequence of contexts and the sequence of conclusions separately so
that (1) they can be compared separately across time and (2) it is
obvious which part of the tactic state is changing at each
step. Indeed, some tactics only change the conclusion, while others
only change the context, while some contexts change both.

Our model will take
\begin{enumerate}
\item A partial-path $p$ through a well-typed tactic tree. Note that
\item For each path $p$, the remaining distance to a terminal node.
\end{enumerate}
Upon embedding the tactic tree, we obtain for each path in the
tactic tree, a sequence of diffs.
\[
\embedenv{TR} = (d_1, \dots, d_k) \,.
\]
For each diff $d_i$ (\ie, forach each path in a tactic tree), we
connect it to a fully-connected layer $FC_1$ (outputs $1$ number)
whose job is to predict the distance left until we reach a terminal
node.
\begin{align*}
  V(p_i) \eqdef \embedenv{TR}_i \otimes FC_1
\end{align*}

Then, we can define a loss for each path as:
\[
L(p_i) = (V(p_i) - \textit{dist}(p_i))^2
\]

\end{document}
